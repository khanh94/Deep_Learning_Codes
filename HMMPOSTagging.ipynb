{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Part of Speech Tagging with Bigram Hidden Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the necessary libraries for opening and processing conllu files are imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pickle as pickle\n",
    "from conllu import parse\n",
    "from conll_df import conll_df\n",
    "import nltk\n",
    "from io import open\n",
    "from collections import defaultdict\n",
    "import pyconll as pcon\n",
    "import itertools\n",
    "import sys\n",
    "import time\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Defining global variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global variables such as the tag sets and the name of the training corpuses are defined here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables for storing part of speech tags and maximum value of log probabilities\n",
    "max_val = - math.log(1.0 / 80000)\n",
    "\n",
    "tag_set = {'ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', \n",
    "'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X'}\n",
    "\n",
    "refined_tag_set = {',', 'POS', 'WP$', 'MD', 'GW', 'EX', 'NN', \n",
    "                   '-LRB-', 'RBR', 'WRB', 'JJR', 'XX', 'RB', 'NNP', \n",
    "                   'NNPS', 'RP', ':', 'JJ', 'CC', 'PRP', 'AFX', 'VB', 'WDT', \n",
    "                   '``', 'NNS', 'WP', 'IN', 'VBP', 'NFP', 'PRP$', '.', 'SYM', \n",
    "                   'DT', 'TO', 'VBZ', 'PDT', 'VBG', 'FW', 'CD', 'HYPH', 'VBD', 'VBN', \n",
    "                   '-RRB-', 'UH', 'LS', 'JJS', 'RBS', 'ADD', \"''\", '$'}\n",
    "\n",
    "eng_train = 'en-ud-train.conllu' \n",
    "eng_dev = 'en-ud-dev.conllu'\n",
    "eng_test = 'en-ud-test.conllu'\n",
    "\n",
    "es_train = 'es-ud-train.conllu'\n",
    "es_dev = 'es-ud-dev.conllu'\n",
    "es_test = 'es-ud-test.conllu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: A processor to process sentences and tag lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First a file processor class is defined to get all the sentences and their respective tags into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilePreprecessor:\n",
    "\tdef __init__(self, train_file_name, dev_file_name, test_file_name, refined):\n",
    "\t\tself.train_file_name = train_file_name\n",
    "\t\tself.dev_file_name = dev_file_name\n",
    "\t\tself.test_file_name = test_file_name\n",
    "\t\tself.refined = refined\n",
    "    \n",
    "    #Obtain all sentences from the conllu file\n",
    "\tdef obtain_sentences(self, file_name):\n",
    "\t\tsentences = pcon.load_from_file(file_name)\n",
    "\t\treturn sentences\n",
    "\n",
    "    #Process all the sentences and store them into a list with their respective tags\n",
    "\tdef preprocess_sentences(self, sentences):\n",
    "\t\tdata_list = []\n",
    "\t\tsentence_list = []\n",
    "\t\tfull_tag_list = []\n",
    "\t\tfor sentence in sentences:\n",
    "\t\t\tword_list = []\n",
    "\t\t\ttag_list = []\n",
    "\t\t\tfor token in sentence:\n",
    "\t\t\t\tword_list.append(token.form)\n",
    "\t\t\t\tif self.refined:\n",
    "\t\t\t\t\ttag_list.append(token.xpos)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttag_list.append(token.upos)\n",
    "            #Convert all words in the list into lower case\n",
    "\t\t\tword_list = [str(x).lower() for x in word_list]\n",
    "\t\t\tp = nltk.PorterStemmer()\n",
    "\t\t\twnl = WordNetLemmatizer()\n",
    "\t\t\tword_list = [wnl.lemmatize(x) for x in word_list]\n",
    "\t\t\tsentence_with_tags = [tuple(word_list), tuple(tag_list)]\n",
    "\t\t\t\n",
    "\t\t\t#Get the list of sentences, list of tags and the combined list\n",
    "\t\t\tsentence_list.append(word_list)\n",
    "\t\t\tfull_tag_list.append(tag_list)\n",
    "\t\t\tdata_list.append(tuple(sentence_with_tags))\n",
    "\t\treturn sentence_list, full_tag_list, data_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Implementation of the Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Viterbi algorithm is then implemented with the calculation of the probabilities and the final evaluation of the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViterbiSolver:\n",
    "\tdef __init__(self, refined=False):\n",
    "    #Store different types of probabilities in dictionaries\n",
    "\t\tself.refined = refined\n",
    "\t\tself.start_prob = defaultdict()\n",
    "\t\tself.tag_set = refined_tag_set if self.refined==True else tag_set\n",
    "\n",
    "\t\tself.tag_prob = dict()\n",
    "\t\tfor tag in self.tag_set:\n",
    "\t\t\tself.tag_prob[tag] = 0\n",
    "\n",
    "\t\tself.transition_prob = defaultdict(dict)\n",
    "\t\tfor row_tag in self.tag_set:\n",
    "\t\t\tfor column_tag in self.tag_set:\n",
    "\t\t\t\tself.transition_prob[row_tag][column_tag] = 0\n",
    "\n",
    "\t\tself.emission_prob = defaultdict(dict)\n",
    "    \n",
    "    #Static method to normalize a dictionary based on the log of all probabilities\n",
    "\t@staticmethod\n",
    "\tdef normalize_dict(numeric_dict):\n",
    "\t\tdict_length = len(set(numeric_dict))\n",
    "\t\ttotal_log = math.log(sum(numeric_dict.values()) + 0.1*dict_length)\n",
    "\t\tfor key, value in numeric_dict.items():\n",
    "\t\t\tif value == 0:\n",
    "\t\t\t\tnumeric_dict[key] = sys.maxsize \n",
    "\t\t\telse:\n",
    "\t\t\t\tnumeric_dict[key] = total_log/math.log(value + 0.1)\n",
    "    \n",
    "    #Calculate the different types of probabilities based on \n",
    "    #the list of sentences and tags\n",
    "\tdef evaluate_probabilities(self, data_list):\n",
    "\t\tfor sentence, tags in data_list:\n",
    "\t\t\tif tags[0] not in self.start_prob and tags[0] != None:\n",
    "\t\t\t\tself.start_prob[tags[0]] = 0.1\n",
    "\t\t\tif tags[0] != None:\n",
    "\t\t\t\tself.start_prob[tags[0]] += 1.1\n",
    "\n",
    "\t\t\tfor index in range(0, len(tags) - 1):\n",
    "\t\t\t\tcurrent_tag = tags[index]\n",
    "\t\t\t\tnext_tag = tags[index + 1]\n",
    "\t\t\t\tif current_tag != None and next_tag != None: \n",
    "\t\t\t\t\tself.transition_prob[current_tag][next_tag] += 1.1\n",
    "\n",
    "\t\t\tfor word, tag in zip(sentence, tags):\n",
    "\t\t\t\tif tag != None:\n",
    "\t\t\t\t\tself.tag_prob[tag] += 1.1\n",
    "\t\t\t\tif tag not in self.emission_prob[word] and tag != None:\n",
    "\t\t\t\t\tself.emission_prob[word][tag] = 0.1\n",
    "\t\t\t\tif tag != None:\n",
    "\t\t\t\t\tself.emission_prob[word][tag] += 1.1\n",
    "        \n",
    "        #Normalize the dictionaries\n",
    "\t\tself.normalize_dict(self.start_prob)\n",
    "\t\t\n",
    "\t\tself.normalize_dict(self.tag_prob)\n",
    "\n",
    "\t\tfor dict_of_tag in self.transition_prob.values():\n",
    "\t\t\tself.normalize_dict(dict_of_tag)\n",
    "\n",
    "\t\tfor dict_of_tag in self.emission_prob.values():\n",
    "\t\t\tfor tag, count in dict_of_tag.items():\n",
    "\t\t\t\tif tag != None:\n",
    "\t\t\t\t\tif count >= 1:\n",
    "\t\t\t\t\t\tdict_of_tag[tag] = (math.log(self.tag_prob[tag] + 0.1)\n",
    "                                         /math.log(count + 0.1))  \n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tdict_of_tag[tag] = sys.maxsize\n",
    "    \n",
    "    #Obtain the emission probabilities for each word and store them in a dictionary\n",
    "\tdef obtain_emission_probabilities(self, word):\n",
    "\t\temission_dict_of_probs = defaultdict() \n",
    "\t\tfor tag in self.tag_set:\n",
    "\t\t\tif word in self.emission_prob and tag in self.emission_prob[word] and tag != None:\n",
    "\t\t\t\temission_dict_of_probs[tag] = self.emission_prob[word][tag]\n",
    "\t\t\telse:\n",
    "\t\t\t\temission_dict_of_probs[tag] = max_val\n",
    "\t\treturn emission_dict_of_probs\n",
    "\n",
    "    #Implementation of the Viterbi algorithm using a 2-D numpy array as the Viterbi matrix\n",
    "\tdef viterbi(self, sentence):\n",
    "\t\t'''params sentence: a list of different words in the sentence\n",
    "\t\treturn output_tags: a list of the tags'''\n",
    "\t\tlist_of_tags = list(self.tag_set)\n",
    "\t\tnum_rows = len(list_of_tags)\n",
    "\t\tnum_cols = len(sentence)\n",
    "\t\tviterbi_matrix = np.empty((num_rows, num_cols), dtype=tuple)\n",
    "        \n",
    "        #Go through each column to update the probability\n",
    "\t\tfor column_index, current_word in enumerate(sentence):\n",
    "\t\t\tcurrent_emission_probs = self.obtain_emission_probabilities(current_word)\n",
    "\t\t\tfor row_index, current_tag in enumerate(list_of_tags):\n",
    "\t\t\t\tif column_index == 0:\n",
    "\t\t\t\t\tif current_word in self.start_prob:\n",
    "\t\t\t\t\t\tstart_prob = self.start_prob[current_tag]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tstart_prob = max_val\n",
    "\t\t\t\t\tviterbi_matrix[row_index][column_index] = (-1, current_emission_probs[current_tag]*start_prob)\n",
    "\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tbest_tuple = (-1, max_val)\n",
    "\t\t\t\t\tfor prev_row_index, prev_tag in enumerate(list_of_tags):\n",
    "\t\t\t\t\t\tprev_prob = viterbi_matrix[prev_row_index][column_index - 1][1]\n",
    "\t\t\t\t\t\tcurrent_prob = prev_prob*current_emission_probs[current_tag]*self.transition_prob[prev_tag][current_tag]\n",
    "\t\t\t\t\t\tif current_prob < best_tuple[1]:\n",
    "\t\t\t\t\t\t\tbest_tuple = (prev_row_index, current_prob)\n",
    "\n",
    "\t\t\t\t\tviterbi_matrix[row_index][column_index] = best_tuple\n",
    "        \n",
    "        #Go through the last column to find the index of the maximum probability\n",
    "\t\t(max_index, max_prob) = (-1, max_val)\n",
    "\t\tfor row in range(num_rows):\n",
    "\t\t\tcurrent_prob = viterbi_matrix[row][num_cols - 1][1] \n",
    "\t\t\tif current_prob < max_prob:\n",
    "\t\t\t\t(max_index, max_prob) = (row, current_prob)\n",
    "        \n",
    "        #Backtrack to obtain the best sequence of tags in the columns\n",
    "\t\toutput_tags = list()\n",
    "\t\tfor col in range(num_cols - 1, 0, -1):\n",
    "\t\t\toutput_tags.insert(0, list_of_tags[max_index])\n",
    "\t\t\tmax_index = viterbi_matrix[max_index][col][0]\n",
    "\t\toutput_tags.insert(0, list_of_tags[max_index])\n",
    "\t\treturn output_tags\n",
    "    \n",
    "    #Method to score the performance of the algorithm on the test corpus\n",
    "\tdef score_test_corpus(self, raw_sentences, raw_tags):\n",
    "\t\tall_viterbi_tags = []\n",
    "\t\tfor sentence in raw_sentences:\n",
    "\t\t\tsentence_tag = self.viterbi(sentence)\n",
    "\t\t\tall_viterbi_tags.append(sentence_tag)\n",
    "\n",
    "\t\tfinal_viterbi_tags = list(itertools.chain.from_iterable(all_viterbi_tags))\n",
    "\t\tfinal_test_tags = list(itertools.chain.from_iterable(raw_tags))\n",
    "\t\ttotal_matches = sum([1 for i, j in zip(final_viterbi_tags, final_test_tags) if i == j])\n",
    "\t\taccuracy = total_matches/len(final_test_tags)\n",
    "\t\treturn total_matches, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: English Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the English portion of the assignment. A sample sentence is obtained and then decoded using the Viterbi algorithm. The accuracy on the development and the test sets are then reported. The corresponding dictionary files are also saved into pickle files. The accuracy of the algorithm on the English test set is 84.4%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags for the sample English sentence: ['ADJ', 'ADJ', 'AUX', 'ADV', 'ADJ']\n",
      "The accuracy on the English development set is : 84.553 %\n",
      "The accuracy on the English test set is : 84.365 %\n"
     ]
    }
   ],
   "source": [
    "#English portion: Load the preprocessor and obtain the sentence with tags\n",
    "eng_preprocessor = FilePreprecessor(eng_train, eng_dev, eng_test, refined=False)\n",
    "\n",
    "eng_train_data = eng_preprocessor.obtain_sentences(eng_train)\n",
    "\n",
    "_, _, train_data_list = eng_preprocessor.preprocess_sentences(eng_train_data)\n",
    "\n",
    "#Calculate the probabilities using the viterbi solver\n",
    "eng_solver = ViterbiSolver(refined=False)\n",
    "\n",
    "eng_solver.evaluate_probabilities(train_data_list)\n",
    "\n",
    "#Get a sample sentence and decode it with Viterbi\n",
    "eng_sample_sentence  = ['I', 'like', 'NLP', 'very', 'much']\n",
    "\n",
    "eng_sample_tags = eng_solver.viterbi(eng_sample_sentence)\n",
    "\n",
    "print('Tags for the sample English sentence: {}'.format(eng_sample_tags))\n",
    "\n",
    "\n",
    "#Obtain the dictionaries of transition and emission probabilities and \n",
    "#save them to pickle files\n",
    "eng_transition = eng_solver.transition_prob \n",
    "\n",
    "eng_emission = eng_solver.emission_prob\n",
    "\n",
    "eng_dicts = [eng_transition, eng_emission]\n",
    "\n",
    "with open('eng_probabilities.pkl', 'wb') as eng_prob:\n",
    "\tpickle.dump(eng_dicts, eng_prob)\n",
    "\n",
    "\n",
    "#Evaluate the performance of the algorithm on the development set\n",
    "dev_data = eng_preprocessor.obtain_sentences(eng_dev)\n",
    "\n",
    "dev_sentences_list, dev_tags_list, _ = eng_preprocessor.preprocess_sentences(dev_data)\n",
    "\n",
    "total_dev_matches, dev_accuracy = eng_solver.score_test_corpus(dev_sentences_list, \n",
    "                                                               dev_tags_list)\n",
    "print('The accuracy on the English development set is : {:.3f} %'\n",
    "      .format(dev_accuracy*100.0))\n",
    "\n",
    "\n",
    "#Test the performance of the algorithm on the test set\n",
    "test_data = eng_preprocessor.obtain_sentences(eng_test)\n",
    "\n",
    "test_sentences_list, test_tags_list, _ = eng_preprocessor.preprocess_sentences(test_data)\n",
    "\n",
    "total_matches, accuracy = eng_solver.score_test_corpus(test_sentences_list, \n",
    "                                                       test_tags_list)\n",
    "print('The accuracy on the English test set is : {:.3f} %'\n",
    "      .format(accuracy*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Spanish Part of Speech Tagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Spanish portion of the assignment. A sample Spanish sentence is obtained and then decoded using the Viterbi algorithm. The accuracy on the development and the test sets are then also reported. The Spanish probability dictionaries are also saved into pickle files. The accuracy of the algorithm on the Spanish test set is 88.2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags for the sample Spanish sentence: ['ADJ', 'AUX', 'VERB', 'PRON', 'PRON']\n",
      "The accuracy on the Spanish development set is : 88.315 %\n",
      "The accuracy on the Spanish test set is : 88.335 %\n"
     ]
    }
   ],
   "source": [
    "#Spanish portion: Load the processor and obtain the sentence with tags\n",
    "es_preprocessor = FilePreprecessor(es_train, es_dev, es_test, refined=False)\n",
    "\n",
    "es_train_data = es_preprocessor.obtain_sentences(es_train)\n",
    "\n",
    "_, _, es_train_data_list = es_preprocessor.preprocess_sentences(es_train_data)\n",
    "\n",
    "\n",
    "#Calculate the probabilities using the viterbi solver\n",
    "es_solver = ViterbiSolver(refined=False)\n",
    "\n",
    "es_solver.evaluate_probabilities(es_train_data_list)\n",
    "\n",
    "\n",
    "#Get a sample Spanish sentence and decode it with Viterbi\n",
    "es_sample_sentence  = ['Yo', 'te', 'quiero', 'demasiado', 'mucho']\n",
    "\n",
    "es_sample_tags = es_solver.viterbi(es_sample_sentence)\n",
    "\n",
    "print('Tags for the sample Spanish sentence: {}'.format(es_sample_tags))\n",
    "\n",
    "\n",
    "#Obtain the dictionaries of transition and emission probabilities \n",
    "#and save them to pickle files\n",
    "es_transition = es_solver.transition_prob \n",
    "\n",
    "es_emission = es_solver.emission_prob\n",
    "\n",
    "es_dicts = [es_transition, es_emission]\n",
    "\n",
    "with open('es_probabilities.pkl', 'wb') as es_prob:\n",
    "\tpickle.dump(es_dicts, es_prob)\n",
    "\n",
    "    \n",
    "#Evaluate the performance of the algorithm on the development set\n",
    "es_dev_data = es_preprocessor.obtain_sentences(es_dev)\n",
    "\n",
    "es_dev_sentences_list, es_dev_tags_list, _ = es_preprocessor.preprocess_sentences(es_dev_data)\n",
    "\n",
    "es_total_dev_matches, es_dev_accuracy = es_solver.score_test_corpus(es_dev_sentences_list, \n",
    "                                              es_dev_tags_list)\n",
    "print('The accuracy on the Spanish development set is : {:.3f} %'\n",
    "      .format(es_dev_accuracy*100.0))\n",
    "\n",
    "\n",
    "#Test the performance of the algorithm on the test set\n",
    "es_test_data = es_preprocessor.obtain_sentences(es_test)\n",
    "\n",
    "es_test_sentences_list, es_test_tags_list, _ = es_preprocessor.preprocess_sentences(es_test_data)\n",
    "\n",
    "es_total_matches, es_accuracy = es_solver.score_test_corpus(es_test_sentences_list, \n",
    "                                          es_test_tags_list)\n",
    "print('The accuracy on the Spanish test set is : {:.3f} %'.format(es_accuracy*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: English Part of Speech Tagging with Refined Tag Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probabilities are calculated for the part of speech tagging using refined tag sets. An English sentence is used to test the tagging using the refined tag set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refined tag set portion\n",
    "ref_preprocessor = FilePreprecessor(eng_train, eng_dev, eng_test, refined=True)\n",
    "\n",
    "ref_train_data = ref_preprocessor.obtain_sentences(eng_train)\n",
    "\n",
    "_, _, ref_train_data_list = ref_preprocessor.preprocess_sentences(ref_train_data)\n",
    "\n",
    "\n",
    "#Calculate the probabilities using the viterbi solver\n",
    "ref_solver = ViterbiSolver(refined=True)\n",
    "\n",
    "ref_solver.evaluate_probabilities(ref_train_data_list)\n",
    "\n",
    "#Get a sample sentence and decode it with Viterbi\n",
    "ref_sample_sentence  = ['I', 'like', 'NLP', 'very', 'much']\n",
    "\n",
    "ref_sample_tags = ref_solver.viterbi(ref_sample_sentence)\n",
    "\n",
    "print('Tags for the English sentence using the refined tag set: {}'\n",
    "      .format(ref_sample_tags))\n",
    "\n",
    "#Obtain the dictionaries of transition and emission probabilities \n",
    "#and save them to pickle files\n",
    "ref_transition = ref_solver.transition_prob \n",
    "\n",
    "ref_emission = ref_solver.emission_prob\n",
    "\n",
    "ref_dicts = [ref_transition, ref_emission]\n",
    "\n",
    "with open('ref_probabilities.pkl', 'wb') as ref_prob:\n",
    "\tpickle.dump(ref_dicts, ref_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Error Analysis and Suggested Solutions to Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the algorithm ranges from 84-90% for both the English and the Spanish corpus. This is a bit low in comparison with the standard baseline of 90-95% accuracy. To improve on this performance various smoothing techniques can be used in the calculation of the emission and transition probabilities for each of the corpus, such as the Laplace smoothing (adding one to the word count and adding the total number of distinct words to the total number of words in the corpus) in the final probability calculation. Assigning a small probability to an unseen word can also improve the accuracy of the probability calculation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
